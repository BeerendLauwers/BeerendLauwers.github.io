<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
        <meta name="google-site-verification" content="-BkNYpagkkm7jEqVwr37d7emdyyiNVbpGwedl92HsC0" />
        <title>Emulating wget's directory structure creation with aria2</title>
        <link rel="stylesheet" type="text/css" href="../css/default.css" />
        <link rel="stylesheet" type="text/css" href="../css/custom.css" />
        <link rel="stylesheet" type="text/css" href="../css/font-awesome.min.css">
        <script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

          ga('create', 'UA-64277210-1', 'auto');
          ga('send', 'pageview');
        </script>
    </head>
    <body>
        <div class="page-wrap">
            <h1>Emulating wget's directory structure creation with aria2</h1>
<p class="author">By <a href="http://beerendlauwers.be/">Beerend Lauwers</a></p>

<p><code>wget</code> is an excellent tool for collecting information from open directories of universities, free data sets, and so on. Unfortunately, it’s also quite old. By itself, this is not a problem, but it does not, for instance, support multiple downloads at once.</p>
<p>Starting and stopping a long-running <code>wget</code> is also fraught with peril, and you must ensure you are using the proper flags to avoid duplicates or overwriting your previous work.</p>
<p>It’s also a purely command-line program with no long-running server process behind it, which means you <em>definitely</em> cannot close the terminal it’s running in, or power down the machine, etc.</p>
<p>This is why many people use <code>aria2</code> instead, which provides a long-running process that can be communicated with over RPC or command-line commands.</p>
<p>However, <code>aria2</code> just takes URLs, downloads them and dumps the resulting files in a folder. On top of that, it doesn’t do any recursive downloading like <code>wget</code> does.</p>
<h2 id="standing-on-top-of-internet-giants">Standing on top of internet giants</h2>
<p>(A.K.A. I googled.)</p>
<p>There are plenty of tools for taking <code>wget</code>’s <code>--spider</code> output and turning it into a list usable by <code>aria2</code>, <a href="https://github.com/kajeagentspi/Datahoarder/blob/master/theripper.sh">theripper.sh</a> being one of them.</p>
<p><a href="https://asdfghjkl.me.uk/blog/apache-to-aria2/">Here’s another</a>. But this blog post actually provided me with an interesting idea: I can just run an <code>aria2</code> server, with <code>aria2-ui</code> enabled and add stuff via the JSON RPC.</p>
<p>Additionally, it also includes code for pausing and resuming downloads at particular times of the day, which is a godsend in countries that have bandwidth caps during the day, but not during the night.</p>
<h2 id="lets-slap-something-together">Let’s slap something together!</h2>
<h3 id="get-a-file-with-a-bunch-of-urls">Get a file with a bunch of URLs</h3>
<p>I edited <code>theripper.sh</code> and commented out the <code>download()</code> and some cleanup parts:</p>
<pre><code>echo &quot;Creating list of urls...&quot;
spider
echo &quot;Index created!&quot;
#download

# Cleanup
rm opendir-$$.log
rm opendir-$$.log.tmp
#rm list-$$.txt
#rm link-$$.down
</code></pre>
<p>(The <code>.log</code> file is useful for <code>tail -f</code>’ing it to see if it’s performing as expected, by the way.)</p>
<h3 id="preprocess-the-list-and-send-it-off-to-aria2">Preprocess the list and send it off to <code>aria2</code></h3>
<p>Next up was a whole bunch of googling for regexes for pulling apart a URL into its constituent parts so I could recreate the directory structure, and putting it in a shell script.</p>
<p>The basis was the shell script from <a href="https://sourceforge.net/p/aria2/discussion/540046/thread/b7e1eb37/?limit=25">this post</a>.</p>
<p>After putting in the regex code for extracting the host name and path from the URL, I tweaked the RPC code from the aforementioned blog post and <a href="https://github.com/beerendlauwers/wget-directories-in-aria2/blob/master/add_files_to_aria2_rpc.sh">ended up with this script</a>. To use it, just run <code>./add_files_to_aria2_rpc.sh URL_LIST_FILE</code> and you’re in business.</p>
<p>Files are added paused so I can download them overnight to prevent impacting my bandwidth cap. You can change this as desired, the RPC interface supports all of <a href="https://aria2.github.io/manual/en/html/aria2c.html#options"><code>aria2</code>’s command-line options</a>.</p>
<p>Be sure to add a secret token and username/password combination for HTTP basic authentication. (And run it over HTTPS if you’re exposing the RPC interface on the internet.)</p>
<h3 id="sidenote-mounting-a-cifs-smb-share-on-startup">Sidenote: mounting a CIFS (SMB) share on startup</h3>
<p>Everything’s dumped to a box that has a SMB share available. To mount it, just add this line to <code>/etc/fstab</code>:</p>
<pre><code>//IP-ADDR/REMOTE/PATH /media/LOCAL/PATH cifs guest,uid=1000,iocharset=utf8,user 0 0</code></pre>
<h3 id="pausing-and-resuming-aria2">Pausing and resuming <code>aria2</code></h3>
<p>Again, borrowed from the second blog post mentioned above:</p>
<pre><code># Edit this file to introduce tasks to be run by cron.
# 
# Each task to run has to be defined through a single line
# indicating with different fields when the task will be run
# and what command to run for the task
# 
# To define the time you can provide concrete values for
# minute (m), hour (h), day of month (dom), month (mon),
# and day of week (dow) or use '*' in these fields (for 'any').# 
# Notice that tasks will be started based on the cron's system
# daemon's notion of time and timezones.
# 
# Output of the crontab jobs (including errors) is sent through
# email to the user the crontab file belongs to (unless redirected).
# 
# For example, you can run a backup of all your user accounts
# at 5 a.m every week with:
# 0 5 * * 1 tar -zcf /var/backups/home.tgz /home/
# 
# For more information see the manual pages of crontab(5) and cron(8)
# 
# m h  dom mon dow   command

# Pause aria2 downloads at 11:30.
30 11 * * * curl http://127.0.0.1:6800/jsonrpc -H &quot;Content-Type: application/json&quot; -H &quot;Accept: application/json&quot; --data '{&quot;jsonrpc&quot;: &quot;2.0&quot;,&quot;id&quot;:1, &quot;method&quot;: &quot;aria2.pauseAll&quot;, &quot;params&quot;:[&quot;token:secret_token&quot;]}'

# Resume aria2 downloads at 00:30.
30 00 * * * curl http://127.0.0.1:6800/jsonrpc -H &quot;Content-Type: application/json&quot; -H &quot;Accept: application/json&quot; --data '{&quot;jsonrpc&quot;: &quot;2.0&quot;,&quot;id&quot;:1, &quot;method&quot;: &quot;aria2.unpauseAll&quot;, &quot;params&quot;:[&quot;token:secret_token&quot;]}'</code></pre>
<p>(Also again: don’t forget your secret token!)</p>
<h3 id="suggestions-improvements">Suggestions? Improvements?</h3>
<p>Feel free to open up an issue or PR on <a href="https://github.com/beerendlauwers/wget-directories-in-aria2/">the Github repository</a>.</p>
<p>Enjoy!</p>

<footer>
  &copy; September  1, 2018 Beerend Lauwers &lt;beerendlauwers@gmail.com&gt;
</footer>

        </div>
        <div class="page-wrap" id="footer">
            Site automatically generated by
            <a href="http://jaspervdj.be/hakyll">Hakyll</a>
        </div>
    </body>
</html>
